{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41bac762-9c50-41b9-bb0a-eda8a38da36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "ROOT_DIR = Path(\".\").absolute().parent.parent\n",
    "# PROJECT_DIR = Path(ROOT_DIR, \"project\")\n",
    "sys.path.append(str(ROOT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62170c9d-9f86-4bdf-b3f8-c8a3dbb132ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f916869d-5f0d-4e0f-ae6b-2ac6377eddb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device `cpu`\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device `{device}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87de4b54-fa4f-40ab-a7af-bd58ed9e0c82",
   "metadata": {},
   "source": [
    "## Starter RAG Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddd66fa-3823-4624-9f34-822700a68c1e",
   "metadata": {},
   "source": [
    "### load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20ac0507-351f-41b4-860e-3655c3177d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d882c323-8656-426e-8477-860e153ed654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Documents processed: 948\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(ROOT_DIR, \"data\")\n",
    "assert DATA_DIR.exists()\n",
    "\n",
    "# read all documentds:\n",
    "with open(Path(DATA_DIR, \"documents.json\"), \"rt\") as f_in:\n",
    "    docs_raw = json.load(f_in)\n",
    "\n",
    "# collect all documents from all the courses\n",
    "documents = []\n",
    "for course_dict in docs_raw:\n",
    "    course_name = course_dict[\"course\"]\n",
    "    for doc in course_dict[\"documents\"]:\n",
    "        doc[\"course\"] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "print(f\"# of Documents processed: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2720ac-97d8-417a-906c-c817fb638061",
   "metadata": {},
   "source": [
    "### Index data with Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f55708d-72b1-4d48-a637-f844d721d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b62a71f-437c-4c06-95f8-50c4c48d5c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"b9dcb48aa018\",\n",
      "  \"cluster_name\" : \"docker-cluster\",\n",
      "  \"cluster_uuid\" : \"fUjwSc3BT9KhhbpJky3U9w\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"8.4.3\",\n",
      "    \"build_flavor\" : \"default\",\n",
      "    \"build_type\" : \"docker\",\n",
      "    \"build_hash\" : \"42f05b9372a9a4a470db3b52817899b99a76ee73\",\n",
      "    \"build_date\" : \"2022-10-04T07:17:24.662462378Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"9.3.0\",\n",
      "    \"minimum_wire_compatibility_version\" : \"7.17.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"7.0.0\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl http://elasticsearch:9200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6eb9fc6-4537-451c-858f-2beea2a06674",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch(\"http://elasticsearch:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "640a3dd9-eed5-44c1-b04a-ae0c820ba9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index successfully created\n"
     ]
    }
   ],
   "source": [
    "# Elastic config definition\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0,\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "index_name = \"course-descripiton\"\n",
    "if es_client.indices.exists(index=index_name):\n",
    "    es_client.indices.delete(index=index_name)\n",
    "\n",
    "# create index\n",
    "try:\n",
    "    es_client.indices.create(\n",
    "        index=index_name,\n",
    "        body=index_settings\n",
    "    )\n",
    "    print(\"Index successfully created\")\n",
    "except:\n",
    "    print(\"Index already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc84b71b-cbcf-4053-8702-5312b31476ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 948/948 [00:01<00:00, 480.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Add document to index one by one:\n",
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "423f39b2-dbb9-4911-aacd-945f067f118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(\n",
    "    query: str,\n",
    "    size: int = 5,\n",
    "    question_weight: int = 3,\n",
    "    course: str = \"machine-learning-zoomcamp\"\n",
    ") -> list[dict]:\n",
    "    \n",
    "    # setup elastic request\n",
    "    search_query = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        # \"fields\": [f\"question^{question_weight}\", \"text\", \"section\"],\n",
    "                        \"fields\": [f\"question^{question_weight}\", \"text\"],\n",
    "                        \"type\": \"best_fields\",\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": f\"{course}\"\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # search in elastic\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    \n",
    "    # save initial documents found as doc list:\n",
    "    result_docs = []\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        result_docs.append(hit[\"_source\"])\n",
    "\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67b2a5-6dff-4cae-8ec3-f8fefa6a58fc",
   "metadata": {},
   "source": [
    "### Index with NOT-Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b32e03d-7116-4ea7-9d2d-94f81e655ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.src.external.minsearch import minsearch  # minsearch.Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d2d35d1-2613-4725-ac6e-5ad000b847ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our search\n",
    "model_index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "model_index.fit(documents);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f43da6fa-b77a-45bc-a135-916a6896acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_search(query: str, num_results: int = 5, course: str = \"data-engineering-zoomcamp\"):\n",
    "    serch_results = model_index.search(\n",
    "        query=query,\n",
    "        filter_dict={\"course\": course},\n",
    "        boost_dict={\"question\": 3.0, \"section\": 0.3},\n",
    "        num_results=num_results\n",
    "    )\n",
    "    return serch_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1b960-b1c9-445a-8f7d-0e526b70c653",
   "metadata": {},
   "source": [
    "### build a Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51c913fb-302a-4819-8877-37de5d6fe06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_template = \"\"\"\n",
    "Q: {question}\n",
    "A: {text}\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_prompt(query: str, search_results: [dict]) -> str:\n",
    "    context_qna = []\n",
    "    for source_doc in search_results:\n",
    "        context_i = context_template.format(\n",
    "            question=source_doc[\"question\"],\n",
    "            text=source_doc[\"text\"],\n",
    "        )\n",
    "        context_qna.append(context_i)\n",
    "\n",
    "    context = \"\\n\\n\".join(context_qna)\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0182b853-f591-463f-95de-0a8fd1fe21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt: str) -> str:\n",
    "    \"\"\"Request LLM to make a query\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15853a8-3832-43b4-bee4-ddc616bf2bc7",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a773674-4082-4157-a6ca-2e9f4f03ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query: str, course: str = \"machine-learning-zoomcamp\") -> str:\n",
    "    # 1. search in elastic\n",
    "    faq_results = index_search(query, course=course)\n",
    "    # 2. build a prompt\n",
    "    prompt = build_prompt(query, search_results=faq_results)\n",
    "    # 3. ask LLM\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab702d-ca16-40b0-9c80-0f854574a503",
   "metadata": {},
   "source": [
    "## HuggingFace Flan-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1b1d6768-c738-403d-99dd-da409c2c2826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f2193d44-2101-4f91-8f90-50b1c1878545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# init HF model:\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af2e50bc-4bd8-49b6-b194-6fdbec346c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: I'm fine.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"how are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=1024)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Output: `{result}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "671c27bc-a921-40d9-8b00-c32add246c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt: str) -> str:\n",
    "    # tokenize prompt:\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # generate output tokens\n",
    "    # Split the input into chunks\n",
    "    chunk_size = 512\n",
    "    input_chunks = [input_ids[i:i+chunk_size] for i in range(0, len(input_ids), chunk_size)]\n",
    "    \n",
    "    # Generate and combine the outputs\n",
    "    outputs = []\n",
    "    for chunk in input_chunks:\n",
    "        chunk = chunk.to(device)\n",
    "        output = model.generate(chunk, max_length=1024)\n",
    "\n",
    "        # decode tokens back to words\n",
    "        outputs.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "    \n",
    "    # Combine the results\n",
    "    answer = \" \".join(outputs)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9b09e6c9-d3b8-469a-bcf0-7070064669f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query: str, course: str = \"machine-learning-zoomcamp\") -> str:\n",
    "    # 1. search in elastic\n",
    "    faq_results = index_search(query, course=course)\n",
    "    # 2. build a prompt\n",
    "    prompt = build_prompt(query, search_results=faq_results)\n",
    "    # 3. ask LLM\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dab2603a-ea33-4c28-9abc-3a3082dcb9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"Is is possible to join the course after it's start?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e0f2e-848a-4d3d-9a5b-f7bb8072b98c",
   "metadata": {},
   "source": [
    "## Phi 3 Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3bbcc104-a894-4e09-b980-d41c08e05cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Downloading shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [07:13<00:00, 216.84s/it]\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.97s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/vgarist/Education/LLM-zoomcamp/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To solve the equation 2x + 3 = 7, follow these steps:\n",
      "\n",
      "1. Subtract 3 from both sides of the equation:\n",
      "   2x + 3 - 3 = 7 - 3\n",
      "   2x = 4\n",
      "\n",
      "2. Divide both sides of the equation by 2:\n",
      "   2x/2 = 4/2\n",
      "   x = 2\n",
      "\n",
      "So, the solution to the equation 2x + 3 = 7 is x = 2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",  \n",
    "    device_map=device,    \n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, \n",
    "]\n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args) \n",
    "answer = output[0]['generated_text']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eb8bbe2e-04e9-4cee-8ac0-c61c1a240db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt: str) -> str:\n",
    "    # tokenize prompt:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    output = pipe(messages, **generation_args) \n",
    "    answer = output[0]['generated_text']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fdf94a50-3452-4feb-aec2-862f9532a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query: str, course: str = \"data-engineering-zoomcamp\") -> str:\n",
    "    # 1. search in elastic\n",
    "    faq_results = index_search(query, course=course)\n",
    "    # 2. build a prompt\n",
    "    prompt = build_prompt(query, search_results=faq_results)\n",
    "    # 3. ask LLM\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "766751e5-970c-4314-9f5e-c2e026e9ea16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes, you can still join the course even if you discover it after the start date. You are eligible to submit homeworks, but remember to meet the deadlines for final projects.'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(query=\"I've just discovered the course. Can I still join?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c4346-5ab6-4db3-8d2a-88e4fb42a828",
   "metadata": {},
   "source": [
    "## Mistral-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "44abeb29-4209-421d-804f-fbd49713d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d6c0d-9117-4da8-83e7-976602cf52f4",
   "metadata": {},
   "source": [
    "## Other open-source LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4799610f-9bb9-4cd4-a950-72b46947670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecaa1b23-cc9e-4a34-a55f-43ecf5a530f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(\n",
    "    base_url=\"http://ollama:11434/v1/\",\n",
    "    api_key=\"ollama\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e33f76-32bd-4f95-9df3-e300c6495177",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client.chat.completions.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac3f2590-32a2-46e6-b5dd-0663aa7ada15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt: str) -> str:\n",
    "    \"\"\"Request to ChatGPT API to make a query.\n",
    "    Possible roles: user, assistant, system\n",
    "    \"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"llama3\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a helpful AI assistant, providing information about the course. \"\n",
    "                    \"Do not provide meta, just answer user questions.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d4a478d4-2257-4175-8475-ad4a9799e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query: str, course: str = \"data-engineering-zoomcamp\") -> str:\n",
    "    # 1. search in elastic\n",
    "    faq_results = elastic_search(query, course=course)\n",
    "    # 2. build a prompt\n",
    "    prompt = build_prompt(query, search_results=faq_results)\n",
    "    # 3. ask LLM\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec938ae1-f834-4557-8c3b-efa2ceec2eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context from the FAQ database, the answer to your question is:\n",
      "\n",
      "Yes, it is possible to join the course now. The FAQ states that even if you don't register ahead of time, you're still eligible to submit the homeworks and can continue with the course at your own pace after it finishes.\n"
     ]
    }
   ],
   "source": [
    "ans = rag(\"I've just found this course. Is it possible to join it now?\")\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d3f64-d45a-42b1-bb55-d7eb5fd2e58e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
